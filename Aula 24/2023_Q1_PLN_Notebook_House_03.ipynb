{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPia8R3bj+EBUbH79mmWD7+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adalves-ufabc/2023.Q1-PLN/blob/main/2023_Q1_PLN_Notebook_House_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2023.Q1]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmK05FgcOzL2"
      },
      "source": [
        "## **PROJETO PRÁTICO [Série House]**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conjunto de Dados**"
      ],
      "metadata": {
        "id": "miwiRondAroI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "from io import BytesIO\n",
        "\n",
        "# ID do arquivo zip no Google Drive\n",
        "file_id = \"1_rAzomhikVwtK32LV_yGhTKGuUsoEQ10\"\n",
        "\n",
        "# URL de download do arquivo zip\n",
        "url = f\"https://drive.google.com/uc?id={file_id}&export=download\"\n",
        "\n",
        "# Faz o download do arquivo zip\n",
        "response = requests.get(url)\n",
        "file_bytes = BytesIO(response.content)\n",
        "\n",
        "# Extrai o arquivo zip\n",
        "with zipfile.ZipFile(file_bytes, \"r\") as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "metadata": {
        "id": "qDwQJuC1_mGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Lê o arquivo Excel em um DataFrame\n",
        "df = pd.read_excel('/content/house_season_1-8.xlsx')\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "Ndo69NTSDhnd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "93daf1a7-6489-41f4-b35b-0efe32dec2d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       season  num_episode  episode           title      director character  \\\n",
              "0           1            1     1.01           Pilot  Bryan Singer   Melanie   \n",
              "1           1            1     1.01           Pilot  Bryan Singer   Rebecca   \n",
              "2           1            1     1.01           Pilot  Bryan Singer   Melanie   \n",
              "3           1            1     1.01           Pilot  Bryan Singer   Rebecca   \n",
              "4           1            1     1.01           Pilot  Bryan Singer   Melanie   \n",
              "...       ...          ...      ...             ...           ...       ...   \n",
              "76460       8          177     8.22  Everybody Dies   David Shore     House   \n",
              "76461       8          177     8.22  Everybody Dies   David Shore    Wilson   \n",
              "76462       8          177     8.22  Everybody Dies   David Shore     House   \n",
              "76463       8          177     8.22  Everybody Dies   David Shore    Wilson   \n",
              "76464       8          177     8.22  Everybody Dies   David Shore     House   \n",
              "\n",
              "                                              transcript  \n",
              "0                                      Why are you late?  \n",
              "1                   You’re not going to like the answer.  \n",
              "2                             I already know the answer.  \n",
              "3                                      I missed the bus.  \n",
              "4      I don’t doubt it, no bus stops near Brad’s. Yo...  \n",
              "...                                                  ...  \n",
              "76460                  Just switched the dental records.  \n",
              "76461  You're destroying your entire life. You can't ...  \n",
              "76462  I'm dead, Wilson. How do you want to spend you...  \n",
              "76463         When the cancer starts getting really bad…  \n",
              "76464                                   Cancer's boring.  \n",
              "\n",
              "[76465 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-85aadb29-362d-44fa-b95e-39cb0902b0c4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>season</th>\n",
              "      <th>num_episode</th>\n",
              "      <th>episode</th>\n",
              "      <th>title</th>\n",
              "      <th>director</th>\n",
              "      <th>character</th>\n",
              "      <th>transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.01</td>\n",
              "      <td>Pilot</td>\n",
              "      <td>Bryan Singer</td>\n",
              "      <td>Melanie</td>\n",
              "      <td>Why are you late?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.01</td>\n",
              "      <td>Pilot</td>\n",
              "      <td>Bryan Singer</td>\n",
              "      <td>Rebecca</td>\n",
              "      <td>You’re not going to like the answer.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.01</td>\n",
              "      <td>Pilot</td>\n",
              "      <td>Bryan Singer</td>\n",
              "      <td>Melanie</td>\n",
              "      <td>I already know the answer.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.01</td>\n",
              "      <td>Pilot</td>\n",
              "      <td>Bryan Singer</td>\n",
              "      <td>Rebecca</td>\n",
              "      <td>I missed the bus.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.01</td>\n",
              "      <td>Pilot</td>\n",
              "      <td>Bryan Singer</td>\n",
              "      <td>Melanie</td>\n",
              "      <td>I don’t doubt it, no bus stops near Brad’s. Yo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76460</th>\n",
              "      <td>8</td>\n",
              "      <td>177</td>\n",
              "      <td>8.22</td>\n",
              "      <td>Everybody Dies</td>\n",
              "      <td>David Shore</td>\n",
              "      <td>House</td>\n",
              "      <td>Just switched the dental records.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76461</th>\n",
              "      <td>8</td>\n",
              "      <td>177</td>\n",
              "      <td>8.22</td>\n",
              "      <td>Everybody Dies</td>\n",
              "      <td>David Shore</td>\n",
              "      <td>Wilson</td>\n",
              "      <td>You're destroying your entire life. You can't ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76462</th>\n",
              "      <td>8</td>\n",
              "      <td>177</td>\n",
              "      <td>8.22</td>\n",
              "      <td>Everybody Dies</td>\n",
              "      <td>David Shore</td>\n",
              "      <td>House</td>\n",
              "      <td>I'm dead, Wilson. How do you want to spend you...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76463</th>\n",
              "      <td>8</td>\n",
              "      <td>177</td>\n",
              "      <td>8.22</td>\n",
              "      <td>Everybody Dies</td>\n",
              "      <td>David Shore</td>\n",
              "      <td>Wilson</td>\n",
              "      <td>When the cancer starts getting really bad…</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76464</th>\n",
              "      <td>8</td>\n",
              "      <td>177</td>\n",
              "      <td>8.22</td>\n",
              "      <td>Everybody Dies</td>\n",
              "      <td>David Shore</td>\n",
              "      <td>House</td>\n",
              "      <td>Cancer's boring.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>76465 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85aadb29-362d-44fa-b95e-39cb0902b0c4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-85aadb29-362d-44fa-b95e-39cb0902b0c4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-85aadb29-362d-44fa-b95e-39cb0902b0c4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POS Tagging**"
      ],
      "metadata": {
        "id": "1yAJ6I2Q-Cnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNndgL-SulyE",
        "outputId": "d3aac032-c21b-4150-8223-b83f9789d123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import spacy\n",
        "\n",
        "def word_freq(df, seasons, tags, n=None):\n",
        "    # Filtra as temporadas desejadas\n",
        "    df_seasons = df[df['season'].isin(seasons)]\n",
        "    \n",
        "    # Junta os textos de cada episódio da temporada selecionada\n",
        "    episodes_text = df_seasons.groupby(['season', 'episode'])['transcript'].apply(lambda x: ' '.join(x)).reset_index()\n",
        "\n",
        "    # Carrega o modelo em inglês do spaCy\n",
        "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "    \n",
        "    # Função para processar o texto e extrair as tags\n",
        "    def extract_nouns(text):\n",
        "        doc = nlp(text)\n",
        "        return [token.text for token in doc if token.pos_ in tags]\n",
        "\n",
        "    # Aplica a função para extrair as tags de cada episódio\n",
        "    episodes_text['tags'] = episodes_text['transcript'].apply(extract_nouns)\n",
        "    \n",
        "    # Une as listas de palavras de todos os episódios em uma única lista\n",
        "    words = [word for episode_words in episodes_text['tags'].tolist() for word in episode_words]\n",
        "\n",
        "    # stopwords em inglês\n",
        "    stop_words = stopwords.words('english')\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Remove as stopwords e lematiza\n",
        "    words_filtered = [lemmatizer.lemmatize(token) for token in words if token.isalpha() and token.lower() not in stop_words]\n",
        "    \n",
        "    # Conta as palavras e pega as N mais frequentes\n",
        "    word_freq = dict(Counter(words_filtered).most_common(n))\n",
        "\n",
        "    # Ordena as palavras em ordem decrescente de frequência\n",
        "    word_freq = dict(sorted(word_freq.items(), key=lambda x: x[1], reverse=True))\n",
        "    \n",
        "    return word_freq"
      ],
      "metadata": {
        "id": "a3s81kC22eaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n palavras mais frequentes em determinadas temporadas de acordo com o(s) tipo(s) de tag(s)\n",
        "word_freq(df, [1,2], ['NOUN'], 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5kokLcB3FSB",
        "outputId": "9a9e5e2a-e380-4ce9-a210-8d48a64b3979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'time': 454,\n",
              " 'patient': 426,\n",
              " 'thing': 420,\n",
              " 'blood': 375,\n",
              " 'guy': 371,\n",
              " 'way': 337,\n",
              " 'test': 317,\n",
              " 'year': 309,\n",
              " 'doctor': 296,\n",
              " 'people': 296}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def find_trasncripts(df, search_terms, seasons, n=None):\n",
        "    # Filter the DataFrame by season and search terms\n",
        "    results = df[df['season'].isin(seasons) & df['transcript'].str.contains('|'.join(search_terms), case=False)]\n",
        "    \n",
        "    # Sort the results by season and episode\n",
        "    results = results.sort_values(['season', 'episode'])\n",
        "    \n",
        "    # Return the top n lines of dialogue\n",
        "    return results['transcript'][:n]"
      ],
      "metadata": {
        "id": "VcgOnGpaRdre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_trasncripts(df, ['what cause'], [1,2,3,4,5,6,7,8], 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtG8OMHcRgul",
        "outputId": "428408fd-35e9-451d-bf22-88a38298e261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5557     Purulent sputum, dyspnea, bronchi bilaterally....\n",
              "5720                         We don’t know what causes it.\n",
              "7161     At this point it doesn’t matter what caused th...\n",
              "7356                          W-what caused my s-symptoms?\n",
              "9291                        And brown.  What causes brown?\n",
              "11705    We think it may have been what caused her car ...\n",
              "11750    Thank you for taking no interest in my mother....\n",
              "11857    So, all we have to answer is what causes a 22-...\n",
              "14328    It's not the surgery, it's the secrecy! What c...\n",
              "15210    If I didn't wake him, I wouldn't have learned ...\n",
              "Name: transcript, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def find_nouns(df, search_terms, tags, seasons=None, n=None):\n",
        "    # Filtre o dataframe pelas temporadas especificadas, se fornecidas\n",
        "    if seasons:\n",
        "        df = df[df['season'].isin(seasons)]\n",
        "\n",
        "    # Pesquise os termos de busca no dataframe e extraia as falas correspondentes\n",
        "    search_results = df[df['transcript'].str.contains('|'.join(search_terms), case=False)]\n",
        "\n",
        "    # Utilize a biblioteca spaCy para identificar os substantivos nas falas correspondentes\n",
        "    nouns = []\n",
        "    for text in search_results['transcript']:\n",
        "        doc = nlp(text)\n",
        "        nouns.extend([token.lemma_ for token in doc if token.pos_ in tags])\n",
        "    \n",
        "    # Use o Counter para contar a frequência dos substantivos encontrados\n",
        "    freq = dict(Counter(nouns))\n",
        "    \n",
        "    # Ordene os termos por frequência e selecione apenas os n termos mais frequentes\n",
        "    sorted_terms = sorted(freq.items(), key=lambda x: x[1], reverse=True)[:n]\n",
        "    sorted_freq = {term: frequency for term, frequency in sorted_terms}\n",
        "    \n",
        "    return sorted_freq"
      ],
      "metadata": {
        "id": "qk6nl4_CTGjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_nouns(df, ['what cause'], ['NOUN'], [1,2,3,4,5,6,7,8], 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6TSkHbrjEyo",
        "outputId": "f66182bf-015e-4223-997f-3d003d45d055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'problem': 13,\n",
              " 'heart': 12,\n",
              " 'lung': 9,\n",
              " 'failure': 9,\n",
              " 'symptom': 8,\n",
              " 'liver': 8,\n",
              " 'kidney': 7,\n",
              " 'pain': 7,\n",
              " 'loss': 6,\n",
              " 'question': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def find_tags_in_titles(df, tags, seasons=None, n=None):\n",
        "    # Filtra o dataframe pelas temporadas especificadas, se fornecidas\n",
        "    if seasons:\n",
        "        df = df[df['season'].isin(seasons)]\n",
        "\n",
        "    # Seleciona apenas as colunas relevantes\n",
        "    df_title = df[['title']]\n",
        "    df_title = df_title.drop_duplicates()['title']\n",
        "\n",
        "    # Utilize a biblioteca spaCy para identificar as palavras de acordo com os tipos de tags nos títulos\n",
        "    words = []\n",
        "    for text in df_title:\n",
        "        doc = nlp(text)\n",
        "        words.extend([token.lemma_ for token in doc if token.pos_ in tags])\n",
        "    \n",
        "    # Use o Counter para contar a frequência das palavras encontradas\n",
        "    freq = dict(Counter(words))\n",
        "    \n",
        "    # Ordene os termos por frequência e selecione apenas os n termos mais frequentes\n",
        "    sorted_terms = sorted(freq.items(), key=lambda x: x[1], reverse=True)[:n]\n",
        "    sorted_freq = {term: frequency for term, frequency in sorted_terms}\n",
        "    \n",
        "    return sorted_freq"
      ],
      "metadata": {
        "id": "NHSNjoHQmG4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_tags_in_titles(df, ['NOUN', 'ADJ', 'PROPN'], [1,2,3,4,5,6,7,8], 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0BOLRlEmLS0",
        "outputId": "3d17469d-4f4a-43f4-bdeb-b358183776d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'House': 6,\n",
              " 'love': 2,\n",
              " 'story': 2,\n",
              " 'TB': 2,\n",
              " 'skin': 2,\n",
              " 'lie': 2,\n",
              " 'Euphoria': 2,\n",
              " 'Será': 2,\n",
              " 'Guy': 2,\n",
              " 'word': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reconhecimento de Entidades Nomeadas**"
      ],
      "metadata": {
        "id": "Xv1rdSBxj6ah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A técnica de Reconhecimento de Entidades Nomeadas (NER) pode ser usada na série \"House\" para identificar nomes próprios de pessoas, lugares e organizações mencionados nas falas dos personagens. Essas entidades podem ser úteis para entender as relações entre personagens, locais onde ocorrem os casos médicos e as organizações envolvidas no tratamento dos pacientes.\n",
        "\n",
        "Por exemplo, o nome do hospital em que a série se passa, Princeton-Plainsboro Teaching Hospital, pode ser identificado como uma entidade nomeada. Além disso, os nomes dos personagens, como Gregory House e seus colegas médicos, também podem ser identificados como entidades nomeadas.\n",
        "\n",
        "A análise de entidades nomeadas pode ser usada para entender melhor as relações entre os personagens e a frequência com que determinados locais ou organizações são mencionados. Isso pode ser útil para identificar os personagens mais relevantes na série, a localização geográfica dos casos médicos e os hospitais ou organizações médicas que são frequentemente mencionados na série.\n"
      ],
      "metadata": {
        "id": "eQx8z2X746Re"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Existem várias maneiras de usar NER na série \"House\". Algumas ideias incluem:\n",
        "\n",
        "   * Identificação de personagens principais e secundários: usando NER, você pode identificar o nome dos personagens principais e secundários da série, bem como suas relações com outros personagens.\n",
        "\n",
        "   * Identificação de locais: a série se passa no Princeton-Plainsboro Teaching Hospital, mas há outros locais que também são mencionados. Usando NER, você pode identificar esses locais e descobrir como eles se relacionam com a trama.\n",
        "   \n",
        "   * Análise de frequência de entidades nomeadas: usando NER, você pode analisar a frequência com que os personagens, locais e organizações são mencionados na série. Isso pode ajudar a identificar personagens ou locais importantes na trama.\n",
        "\n",
        "   * Identificação de terminologia médica: a série usa muita terminologia médica, e NER pode ajudar a identificar esses termos e seu significado na trama.\n",
        "\n",
        "   * Análise de sentimentos: além da identificação de entidades nomeadas, o NER também pode ser usado para analisar os sentimentos associados a essas entidades. Isso pode ajudar a entender melhor como os personagens se sentem sobre outros personagens, locais ou organizações."
      ],
      "metadata": {
        "id": "1CN3SHMz55F0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`spaCy Transformers — roBERTa`**"
      ],
      "metadata": {
        "id": "LmSwRPF3CM0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the spacy transformer (roberta-base) english model\n",
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oEjiBJ_9sON",
        "outputId": "0208b25b-30e8-4949-e3d1-bf24060370f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-24 17:10:51.708422: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-trf==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.5.0/en_core_web_trf-3.5.0-py3-none-any.whl (460.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.3/460.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from en-core-web-trf==3.5.0) (3.5.2)\n",
            "Collecting spacy-transformers<1.3.0,>=1.2.0.dev0\n",
            "  Downloading spacy_transformers-1.2.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.7/193.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (67.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (23.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (3.0.12)\n",
            "Collecting transformers<4.29.0,>=3.4.0\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy-alignments<1.0.0,>=0.7.2\n",
            "  Downloading spacy_alignments-0.9.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.9/dist-packages (from spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (3.11.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (3.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (16.0.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.0-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.2/224.2 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers<4.29.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<4.29.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (2022.10.31)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers<4.29.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (2023.4.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (1.3.0)\n",
            "Installing collected packages: tokenizers, spacy-alignments, huggingface-hub, transformers, spacy-transformers, en-core-web-trf\n",
            "Successfully installed en-core-web-trf-3.5.0 huggingface-hub-0.14.0 spacy-alignments-0.9.0 spacy-transformers-1.2.3 tokenizers-0.13.3 transformers-4.28.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import spacy_transformers\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_trf\")"
      ],
      "metadata": {
        "id": "uygNMkHq6SGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_named_entities_from_character(df, character, episode, types_entities):\n",
        "    # Filtra o DataFrame pelas falas do personagem especificado no episódio especificado\n",
        "    filtered_df = df[(df['episode'] == episode) & (df['character'] == character)]\n",
        "    \n",
        "    # Extrai as falas do personagem\n",
        "    transcripts = filtered_df['transcript'].tolist()\n",
        "    \n",
        "    # Processa as falas com o modelo de linguagem do spaCy\n",
        "    docs = [nlp(transcript) for transcript in transcripts]\n",
        "    \n",
        "    # Itera pelas entidades nomeadas e retorna um dicionário com as entidades e seus tipos\n",
        "    named_entities = {}\n",
        "    for doc in docs:\n",
        "      for entity in doc.ents:\n",
        "        if entity.label_ in types_entities:\n",
        "            if entity.text not in named_entities:\n",
        "                named_entities[entity.text] = set([entity.label_])\n",
        "            else:\n",
        "                named_entities[entity.text].add(entity.label_)\n",
        "    \n",
        "    return named_entities"
      ],
      "metadata": {
        "id": "JTl7qVMx6_FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_named_entities_from_character(df, 'House', 1.01, ['PERSON', 'ORG', 'GPE'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WecMKvl--UEn",
        "outputId": "5b8f7218-1532-4084-8689-48e71d128fda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'HMO': {'ORG'},\n",
              " 'Trenton': {'GPE'},\n",
              " 'Jagger': {'PERSON'},\n",
              " 'House': {'ORG', 'PERSON'},\n",
              " 'Wilson': {'PERSON'},\n",
              " 'General Hospital': {'ORG'},\n",
              " 'the New England Journal of Medicine': {'ORG'},\n",
              " 'Felker': {'PERSON'},\n",
              " 'Neurologist': {'PERSON'},\n",
              " 'Chase': {'PERSON'}}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_named_entities_from_character_with_type_and_freq(df, character, episode, types_entities):\n",
        "    # Filtra o DataFrame pelas falas do personagem especificado no episódio especificado\n",
        "    filtered_df = df[(df['episode'] == episode) & (df['character'] == character)]\n",
        "    \n",
        "    # Extrai as falas do personagem\n",
        "    transcripts = filtered_df['transcript'].tolist()\n",
        "    \n",
        "    # Processa as falas com o modelo de linguagem do spaCy\n",
        "    docs = [nlp(transcript) for transcript in transcripts]\n",
        "    \n",
        "    # Itera pelas entidades nomeadas e retorna um dicionário com as entidades, seus tipos e suas frequências de ocorrência\n",
        "    named_entities = {}\n",
        "    for doc in docs:\n",
        "        for entity in doc.ents:\n",
        "            if entity.label_ in types_entities:\n",
        "                named_entity = entity.text\n",
        "                named_entity_type = entity.label_\n",
        "                if named_entity_type not in named_entities:\n",
        "                    named_entities[named_entity_type] = {}\n",
        "                if named_entity not in named_entities[named_entity_type]:\n",
        "                    named_entities[named_entity_type][named_entity] = 1\n",
        "                else:\n",
        "                    named_entities[named_entity_type][named_entity] += 1\n",
        "    \n",
        "    return named_entities"
      ],
      "metadata": {
        "id": "hz61Xs_r8vzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('GPE')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "n4fuTgATAtpq",
        "outputId": "09fe4562-a63d-4369-d2d8-55924b7ce829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Countries, cities, states'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "named_entities = extract_named_entities_from_character_with_type_and_freq(df, 'House', 1.01, ['PERSON', 'ORG', 'GPE'])\n",
        "named_entities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwdpslJ984YV",
        "outputId": "43ade5b1-d946-4acf-9106-de33587d6022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ORG': {'HMO': 1,\n",
              "  'House': 1,\n",
              "  'General Hospital': 1,\n",
              "  'the New England Journal of Medicine': 1},\n",
              " 'GPE': {'Trenton': 1},\n",
              " 'PERSON': {'Jagger': 1,\n",
              "  'Wilson': 1,\n",
              "  'House': 2,\n",
              "  'Felker': 1,\n",
              "  'Neurologist': 1,\n",
              "  'Chase': 2}}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "named_entities['PERSON']['House']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDTXGrvSSpfB",
        "outputId": "adf99fc3-9a23-4495-fc13-78599d9de0f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "named_entities.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EDiu22HTCu2",
        "outputId": "df48c8c4-c511-4d55-f736-466c8ab0662a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['ORG', 'GPE', 'PERSON'])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tipos_entidades = list(named_entities.keys())\n",
        "tipos_entidades[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "96np8t5oTKX6",
        "outputId": "3b79a01f-c5f7-4f01-e2e6-0c02538743df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ORG'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "named_entities['PERSON']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCh_Aw5ITY6K",
        "outputId": "548c7670-adc3-4054-8c6e-c1fdd647ce3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Jagger': 1,\n",
              " 'Wilson': 1,\n",
              " 'House': 2,\n",
              " 'Felker': 1,\n",
              " 'Neurologist': 1,\n",
              " 'Chase': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textos_org = list(named_entities['PERSON'].keys())\n",
        "textos_org"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD0RYwD9Tg7g",
        "outputId": "793e4d37-a4eb-4d6e-eb14-b98880f26b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Jagger', 'Wilson', 'House', 'Felker', 'Neurologist', 'Chase']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valores_org = list(named_entities['PERSON'].values())\n",
        "valores_org"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcTIO9OmTl42",
        "outputId": "7f388593-b5d9-4da9-e821-a0123aafe3fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 2, 1, 1, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`Transformers`**"
      ],
      "metadata": {
        "id": "kFn_7mt4B_13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_FKbA0ST-AB",
        "outputId": "e4fd392c-c9f1-48e9-f8c9-5e0bdbfc63ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.0-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.2/224.2 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.0 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "nlp = pipeline(\"ner\", model=\"Davlan/distilbert-base-multilingual-cased-ner-hrl\")"
      ],
      "metadata": {
        "id": "GS80eQVrT5pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_named_entities_from_character_transformers(df, character, episode):\n",
        "    # Filtra o DataFrame pelas falas do personagem especificado no episódio especificado\n",
        "    filtered_df = df[(df['episode'] == episode) & (df['character'] == character)]\n",
        "    \n",
        "    # Extrai as falas do personagem\n",
        "    transcripts = filtered_df['transcript'].tolist()\n",
        "    \n",
        "    # Processa as falas com o modelo de linguagem do Transformers\n",
        "    docs = nlp(transcripts)\n",
        "    \n",
        "    # Cria uma lista vazia para armazenar as entidades nomeadas completas\n",
        "    named_entities = []\n",
        "\n",
        "    # Itera pelas entidades nomeadas e adiciona informações relevantes a um dicionário\n",
        "    for doc in docs:\n",
        "        for entity in doc:\n",
        "            if entity['entity'] != 'O':\n",
        "                entity_dict = {\n",
        "                    'entity': entity['entity'],\n",
        "                    'score': entity['score'],\n",
        "                    'index': entity['index'],\n",
        "                    'word': entity['word'],\n",
        "                    'start': entity['start'],\n",
        "                    'end': entity['end']\n",
        "                }\n",
        "                named_entities.append(entity_dict)\n",
        "    \n",
        "    return named_entities"
      ],
      "metadata": {
        "id": "FpLCizQrU5ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities = extract_named_entities_from_character_transformers(df, 'House', 1.01)\n",
        "\n",
        "for entity in entities:\n",
        "  print(entity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiuOr8cXUP_g",
        "outputId": "faf171f3-b6f3-4e36-a009-d13ac2af3765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'entity': 'B-LOC', 'score': 0.9996612, 'index': 43, 'word': 'Trento', 'start': 159, 'end': 165}\n",
            "{'entity': 'I-LOC', 'score': 0.9993304, 'index': 44, 'word': '##n', 'start': 165, 'end': 166}\n",
            "{'entity': 'B-LOC', 'score': 0.8416196, 'index': 1, 'word': 'Nice', 'start': 0, 'end': 4}\n",
            "{'entity': 'B-PER', 'score': 0.99985504, 'index': 6, 'word': 'Jagger', 'start': 27, 'end': 33}\n",
            "{'entity': 'B-PER', 'score': 0.86701584, 'index': 12, 'word': 'Duck', 'start': 25, 'end': 29}\n",
            "{'entity': 'I-PER', 'score': 0.7007596, 'index': 13, 'word': '##lings', 'start': 29, 'end': 34}\n",
            "{'entity': 'B-PER', 'score': 0.9983901, 'index': 16, 'word': 'House', 'start': 42, 'end': 47}\n",
            "{'entity': 'B-PER', 'score': 0.9992774, 'index': 19, 'word': 'Wilson', 'start': 57, 'end': 63}\n",
            "{'entity': 'B-PER', 'score': 0.8902711, 'index': 7, 'word': 'House', 'start': 13, 'end': 18}\n",
            "{'entity': 'B-LOC', 'score': 0.9982241, 'index': 28, 'word': 'General', 'start': 91, 'end': 98}\n",
            "{'entity': 'I-LOC', 'score': 0.99948525, 'index': 29, 'word': 'Hospital', 'start': 99, 'end': 107}\n",
            "{'entity': 'B-ORG', 'score': 0.99960166, 'index': 11, 'word': 'New', 'start': 48, 'end': 51}\n",
            "{'entity': 'I-ORG', 'score': 0.9995827, 'index': 12, 'word': 'England', 'start': 52, 'end': 59}\n",
            "{'entity': 'I-ORG', 'score': 0.9997069, 'index': 13, 'word': 'Journal', 'start': 60, 'end': 67}\n",
            "{'entity': 'I-ORG', 'score': 0.9997303, 'index': 14, 'word': 'of', 'start': 68, 'end': 70}\n",
            "{'entity': 'I-ORG', 'score': 0.999816, 'index': 15, 'word': 'Medicine', 'start': 71, 'end': 79}\n",
            "{'entity': 'B-ORG', 'score': 0.9997682, 'index': 10, 'word': 'Met', 'start': 30, 'end': 33}\n",
            "{'entity': 'I-ORG', 'score': 0.9997547, 'index': 11, 'word': '##h', 'start': 33, 'end': 34}\n",
            "{'entity': 'I-ORG', 'score': 0.9997931, 'index': 12, 'word': 'Lab', 'start': 35, 'end': 38}\n",
            "{'entity': 'B-PER', 'score': 0.98383015, 'index': 11, 'word': 'Fel', 'start': 32, 'end': 35}\n",
            "{'entity': 'B-PER', 'score': 0.563545, 'index': 12, 'word': '##ker', 'start': 35, 'end': 38}\n",
            "{'entity': 'B-PER', 'score': 0.9917583, 'index': 6, 'word': 'House', 'start': 8, 'end': 13}\n",
            "{'entity': 'B-PER', 'score': 0.9984926, 'index': 31, 'word': 'Chase', 'start': 108, 'end': 113}\n",
            "{'entity': 'B-PER', 'score': 0.9994529, 'index': 35, 'word': 'Chase', 'start': 108, 'end': 113}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_complete_entities(entities):\n",
        "    # lista de entidades nomeadas completas\n",
        "    complete_entities = []\n",
        "\n",
        "    # entidade atualmente em construção\n",
        "    current_entity = None\n",
        "\n",
        "    # iterar sobre as entidades previstas\n",
        "    for entity in entities:\n",
        "        # se a entidade atual é nula ou a nova entidade começa com \"B-\"\n",
        "        if current_entity is None or entity['entity'][0] == 'B':\n",
        "            # armazena a entidade anterior se existir\n",
        "            if current_entity is not None:\n",
        "                if \"#\" in current_entity['word']:\n",
        "                    current_entity['word'] = current_entity['word'].replace(\"#\", \"\")\n",
        "                complete_entities.append(current_entity)\n",
        "            # começa a construir uma nova entidade\n",
        "            current_entity = {\n",
        "                'entity': entity['entity'][2:],\n",
        "                'score': entity['score'],\n",
        "                'start': entity['start'],\n",
        "                'end': entity['end'],\n",
        "                'word': entity['word'].replace(\"#\", \"\")\n",
        "            }\n",
        "        # se a nova entidade começa com \"I-\" e a entidade atual existe\n",
        "        elif current_entity is not None and entity['entity'][0] == 'I':\n",
        "            # adiciona a palavra à entidade atual\n",
        "            if \"#\" in entity['word']:\n",
        "                current_entity['word'] += entity['word'].replace(\"#\", \"\")\n",
        "            else:\n",
        "                current_entity['word'] += ' ' + entity['word']\n",
        "\n",
        "            # atualiza o valor de \"end\" da nova entidade\n",
        "            current_entity['end'] = entity['end']                \n",
        "\n",
        "    # armazena a última entidade atual se existir\n",
        "    if current_entity is not None:\n",
        "        if \"#\" in current_entity['word']:\n",
        "            current_entity['word'] = current_entity['word'].replace(\"#\", \"\")\n",
        "        complete_entities.append(current_entity)\n",
        "\n",
        "    # retorna as entidades completas\n",
        "    return complete_entities"
      ],
      "metadata": {
        "id": "1JWDc_p9xxBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete_entities = get_complete_entities(entities)\n",
        "\n",
        "for entity in complete_entities:\n",
        "  print(entity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnrAK1eCx0il",
        "outputId": "6007cd88-4b2d-4f47-d175-17b771690c47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'entity': 'LOC', 'score': 0.9996612, 'start': 159, 'end': 166, 'word': 'Trenton'}\n",
            "{'entity': 'LOC', 'score': 0.8416196, 'start': 0, 'end': 4, 'word': 'Nice'}\n",
            "{'entity': 'PER', 'score': 0.99985504, 'start': 27, 'end': 33, 'word': 'Jagger'}\n",
            "{'entity': 'PER', 'score': 0.86701584, 'start': 25, 'end': 34, 'word': 'Ducklings'}\n",
            "{'entity': 'PER', 'score': 0.9983901, 'start': 42, 'end': 47, 'word': 'House'}\n",
            "{'entity': 'PER', 'score': 0.9992774, 'start': 57, 'end': 63, 'word': 'Wilson'}\n",
            "{'entity': 'PER', 'score': 0.8902711, 'start': 13, 'end': 18, 'word': 'House'}\n",
            "{'entity': 'LOC', 'score': 0.9982241, 'start': 91, 'end': 107, 'word': 'General Hospital'}\n",
            "{'entity': 'ORG', 'score': 0.99960166, 'start': 48, 'end': 79, 'word': 'New England Journal of Medicine'}\n",
            "{'entity': 'ORG', 'score': 0.9997682, 'start': 30, 'end': 38, 'word': 'Meth Lab'}\n",
            "{'entity': 'PER', 'score': 0.98383015, 'start': 32, 'end': 35, 'word': 'Fel'}\n",
            "{'entity': 'PER', 'score': 0.563545, 'start': 35, 'end': 38, 'word': 'ker'}\n",
            "{'entity': 'PER', 'score': 0.9917583, 'start': 8, 'end': 13, 'word': 'House'}\n",
            "{'entity': 'PER', 'score': 0.9984926, 'start': 108, 'end': 113, 'word': 'Chase'}\n",
            "{'entity': 'PER', 'score': 0.9994529, 'start': 108, 'end': 113, 'word': 'Chase'}\n"
          ]
        }
      ]
    }
  ]
}